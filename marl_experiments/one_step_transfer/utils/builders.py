"""Utility functions for the OneStepTransfer experiment."""
import functools
from typing import List

import haiku as hk
import jax
import launchpad as lp
import optax
import reverb
import tree
from ml_collections import config_dict

from marl import _types, games, services, utils, worlds
from marl.services import arenas
from marl.services.replay.reverb import adders as reverb_adders
from marl.utils import loggers, node_utils, wrappers
from marl.utils.loggers import terminal as terminal_logger_lib
from marl_experiments.gathering import services as gathering_services
from marl_experiments.gathering.world_model_game_proxy import WorldModelGameProxy
from marl_experiments.one_step_transfer.services import (
    learner_update_with_warm_start,
    render_arena,
    world_model_learner,
)


def _data_to_string(x):
    """Render a datalog to string."""
    return terminal_logger_lib.data_to_string(x, "\n")


def build_game():
    """Game factory function."""
    game = games.Gathering(
        n_agents=2,
        map_name="default_small",
        global_observation=False,
        viewbox_width=10,
        viewbox_depth=10,
    )
    game = wrappers.TimeLimit(game, num_steps=100)
    return game


@node_utils.build_courier_node
def build_counter() -> services.Counter:
    """Builds a counter."""
    return services.Counter()


@node_utils.build_courier_node
def build_steps_limiter(counter, max_steps, step_key) -> services.StepsLimiter:
    """Builds a step limiting service."""
    local_counter = services.Counter(parent=counter)
    return services.StepsLimiter(local_counter, max_steps, step_key)


@node_utils.build_courier_node
def build_stopper() -> services.Stopper:
    """Builds a service for stopping launchpad programs."""
    return services.Stopper()


@node_utils.build_courier_node
def build_snapshot_node(
    snapshot_template: services.Snapshot,
    learner_update_handle: lp.CourierHandle,
    result_dir: utils.ResultDirectory,
):
    """Builds a node for snapshotting parameters periodically."""
    return services.Snapshotter(
        variable_source=learner_update_handle,
        snapshot_templates={"params": snapshot_template},
        directory=result_dir.dir,
        max_to_keep=5,
    )


def build_agent_replay(
    env_spec: worlds.EnvironmentSpec,
    state_and_extra_spec: worlds.TreeSpec,
    replay_config: config_dict.ConfigDict,
) -> lp.ReverbNode:
    """Builds a replay buffer for storing trajectories used to train policies.

    Args:
        config: Configuration settings for the replay buffer containing the fields:
            * samples_per_insert
            * min_size_to_sample
            * error_buffer
            * replay_max_size
            * max_times_sampled
            * sequence_length
            * replay_table_name
        env_spec: Specification for the environment.
        state_and_extra_spec: Specification for the state and extra information generated by the agent.

    Returns:
        Reverb node.
    """

    def _build_reverb_node(
        env_spec: worlds.EnvironmentSpec, sequence_length: int, table_name: str
    ) -> List[reverb.Table]:
        """Utility function for closing the config."""
        signature = reverb_adders.SequenceAdder.signature(
            env_spec,
            state_and_extra_spec,
            sequence_length=sequence_length,
        )
        rate_limiter = reverb.rate_limiters.SampleToInsertRatio(
            samples_per_insert=replay_config.samples_per_insert,
            min_size_to_sample=replay_config.min_size_to_sample,
            error_buffer=replay_config.error_buffer,
        )
        replay_table = reverb.Table(
            name=table_name,
            sampler=reverb.selectors.Uniform(),
            remover=reverb.selectors.Fifo(),
            max_size=replay_config.replay_max_size,
            max_times_sampled=replay_config.max_times_sampled,
            rate_limiter=rate_limiter,
            signature=signature,
        )
        return [replay_table]

    build_reverb_node_fn = functools.partial(
        _build_reverb_node,
        env_spec=env_spec,
        sequence_length=replay_config.sequence_length,
        table_name=replay_config.replay_table_name,
    )
    return lp.ReverbNode(build_reverb_node_fn)


def build_world_replay(config: config_dict.ConfigDict, env_spec: worlds.EnvironmentSpec):
    """Build a replay buffer used to train the world model."""

    def _build_reverb_node(env_spec: worlds.EnvironmentSpec, table_name: str) -> List[reverb.Table]:
        """Closure for building a reverb node."""
        env_spec = tree.map_structure(
            lambda x: worlds.ArraySpec(shape=(2,) + x.shape, dtype=x.dtype, name=x.name), env_spec
        )
        signature = signature = reverb_adders.SequenceAdder.signature(
            environment_spec=env_spec,
            sequence_length=config.sequence_length,
        )
        rate_limiter = reverb.rate_limiters.MinSize(1)
        replay_table = reverb.Table(
            name=table_name,
            sampler=reverb.selectors.Uniform(),
            remover=reverb.selectors.Fifo(),
            max_size=config.replay_max_size,
            max_times_sampled=-1,
            rate_limiter=rate_limiter,
            signature=signature,
        )
        return [replay_table]

    build_reverb_node_fn = functools.partial(
        _build_reverb_node,
        env_spec=env_spec,
        table_name=config.replay_table_name,
    )
    return lp.ReverbNode(build_reverb_node_fn)


@node_utils.build_courier_node(disable_run=True)
def build_agent_training_arena(
    random_key: jax.random.PRNGKey,
    step_key: str,
    policy_graph: hk.Transformed,
    initial_state_graph: hk.Transformed,
    replay: lp.CourierHandle,
    replay_config: config_dict.ConfigDict,
    learner: lp.CourierHandle,
    counter: lp.CourierHandle,
    game: worlds.Game,
    bots,
    result_dir: utils.ResultDirectory,
) -> arenas.TrainingArena:
    """Builds an arena that generates data for training agent policies."""
    reverb_adder = reverb_adders.SequenceAdder(
        client=replay,
        priority_fns={replay_config.replay_table_name: None},
        period=replay_config.sequence_period or (replay_config.sequence_length - 1),
        sequence_length=replay_config.sequence_length,
    )
    # Variable client is responsible for syncing with the Updating node, but does not tell
    # that node when it should update.
    variable_source = services.VariableClient(source=learner)
    policy = services.LearnerPolicy(
        policy_fn=policy_graph,
        initial_state_fn=initial_state_graph,
        reverb_adder=reverb_adder,
        variable_source=variable_source,
        random_key=random_key,
        backend="cpu",
    )
    local_counter = services.Counter(parent=counter)

    # TODO(maxsmith): Currently this assumes the learner is always player 0, generalize.
    players = bots
    players[0] = policy

    logger = loggers.TensorboardLogger(result_dir.dir)
    train_arena = arenas.TrainingArena(
        game=game,
        players=players,
        logger=logger,
        counter=local_counter,
        step_key=step_key,
    )
    return train_arena


@node_utils.build_courier_node(disable_run=True)
def build_agent_planning_arena(
    config,
    random_key: hk.PRNGSequence,
    policy_graph: hk.Transformed,
    initial_state_graph: hk.Transformed,
    replay: lp.CourierHandle,
    learner: lp.CourierHandle,
    counter: lp.CourierHandle,
    game,
    transition_graph,
    world_init_graph,
    world_model_source: lp.CourierHandle,
    opponent_variable_source: lp.CourierHandle,
    result_dir: utils.ResultDirectory,
) -> arenas.TrainingArena:
    """Builds an arena that generates data for training agent policies."""
    players = {}
    key0, key1, key3 = jax.random.split(random_key, num=3)

    # TODO(maxsmith): Currently this assumes the learner is always player 0, generalize.
    reverb_adder = reverb_adders.SequenceAdder(
        client=replay,
        priority_fns={config.replay.replay_table_name: None},
        period=config.replay.sequence_period or (config.replay.sequence_length - 1),
        sequence_length=config.replay.sequence_length,
    )
    # Variable client is responsible for syncing with the Updating node, but does not tell
    # that node when it should update.
    variable_source = services.VariableClient(source=learner)
    policy = services.LearnerPolicy(
        policy_fn=policy_graph,
        initial_state_fn=initial_state_graph,
        reverb_adder=reverb_adder,
        variable_source=variable_source,
        random_key=key0,
        backend="cpu",
    )
    players[0] = policy

    # TODO(maxsmith): This assumes that the opponent has the same graph as the player.
    opponent = services.EvaluationPolicy(
        policy_fn=policy_graph,
        initial_state_fn=initial_state_graph,
        variable_source=services.VariableClient(source=opponent_variable_source),
        random_key=key1,
        backend="cpu",
    )
    players[1] = opponent

    # World model.
    wrapped_model = WorldModelGameProxy(
        transition=transition_graph,
        init_state=world_init_graph,
        variable_source=services.VariableClient(source=world_model_source),
        random_key=key3,
        game=game,
    )
    wrapped_model = wrappers.TimeLimit(wrapped_model, num_steps=100)

    logger = loggers.TensorboardLogger(result_dir.dir)
    local_counter = services.Counter(parent=counter)
    train_arena = arenas.TrainingArena(
        game=wrapped_model,
        players=players,
        logger=logger,
        counter=local_counter,
        step_key=config.step_key,
    )
    return train_arena


@node_utils.build_courier_node(disable_run=True)
def build_agent_planning_arena_without_var_servers(
    step_key: str,
    random_key: hk.PRNGSequence,
    policy_graph: hk.Transformed,
    initial_state_graph: hk.Transformed,
    replay: lp.CourierHandle,
    replay_config: config_dict.ConfigDict,
    learner: lp.CourierHandle,
    counter: lp.CourierHandle,
    game,
    players,
    transition_graph,
    world_init_graph,
    world_model_params: _types.Tree,
    result_dir: utils.ResultDirectory,
) -> arenas.TrainingArena:
    """Builds an arena that generates data for training agent policies.

    This version of planning arenas does not use variable servers to maintain accurate
    an world model and coplayers.
    """
    key0, key1, key3 = jax.random.split(random_key, num=3)

    # TODO(maxsmith): Currently this assumes the learner is always player 0, generalize.
    reverb_adder = reverb_adders.SequenceAdder(
        client=replay,
        priority_fns={replay_config.replay_table_name: None},
        period=replay_config.sequence_period or (replay_config.sequence_length - 1),
        sequence_length=replay_config.sequence_length,
    )
    # Variable client is responsible for syncing with the Updating node, but does not tell
    # that node when it should update.
    variable_source = services.VariableClient(source=learner)
    policy = services.LearnerPolicy(
        policy_fn=policy_graph,
        initial_state_fn=initial_state_graph,
        reverb_adder=reverb_adder,
        variable_source=variable_source,
        random_key=key0,
        backend="cpu",
    )
    players[0] = policy

    # World model.
    wrapped_model = WorldModelGameProxy(
        transition=transition_graph,
        init_state=world_init_graph,
        params=world_model_params,
        random_key=key3,
        game=game,
    )
    wrapped_model = wrappers.TimeLimit(wrapped_model, num_steps=100)

    logger = loggers.TensorboardLogger(result_dir.dir)
    local_counter = services.Counter(parent=counter)
    train_arena = arenas.TrainingArena(
        game=wrapped_model,
        players=players,
        logger=logger,
        counter=local_counter,
        step_key=step_key,
    )
    return train_arena


@node_utils.build_courier_node(disable_run=True)
def build_evaluation_arena_node(
    step_key: str,
    eval_frequency: int,
    policy_graph: hk.Transformed,
    initial_state_graph: hk.Transformed,
    random_key: jax.random.PRNGKey,
    learner: lp.CourierHandle,
    bots,
    counter: lp.CourierHandle,
    game_ctor,
    snapshot_template: services.Snapshot,
    result_dir: utils.ResultDirectory,
):
    """Builds an evaluation arena for a learner that does not collect experiences."""
    variable_source = services.VariableClient(source=learner)
    evaluation_policy = services.EvaluationPolicy(
        policy_fn=policy_graph,
        initial_state_fn=initial_state_graph,
        variable_source=variable_source,
        random_key=random_key,
    )
    logger = loggers.TensorboardLogger(result_dir.dir)
    local_counter = services.Counter(parent=counter)

    snapshotter = services.PrioritySnapshotter(
        snapshot_template=snapshot_template,
        directory=result_dir.dir,
        max_to_keep=3,
    )

    return arenas.EvaluationArena(
        agents={0: evaluation_policy},
        bots=bots,
        scenarios=arenas.evaluation_arena.EvaluationScenario(game_ctor=game_ctor, num_episodes=5),
        evaluation_frequency=eval_frequency,
        counter=local_counter,
        logger=logger,
        step_key=step_key,
        snapshotter=snapshotter,
    )


@node_utils.build_courier_node(disable_run=False)
def build_agent_vs_agent_training_arena(
    config,
    random_key: hk.PRNGSequence,
    policy_graph: hk.Transformed,
    initial_state_graph: hk.Transformed,
    replay: lp.CourierHandle,
    learner: lp.CourierHandle,
    counter: lp.CourierHandle,
    game: worlds.Game,
    opponent_variable_source: lp.CourierHandle,
    result_dir: utils.ResultDirectory,
) -> arenas.TrainingArena:
    """Builds an arena that generates data for training agent policies."""
    players = {}
    key0, key1 = jax.random.split(random_key)

    # TODO(maxsmith): Currently this assumes the learner is always player 0, generalize.
    reverb_adder = reverb_adders.SequenceAdder(
        client=replay,
        priority_fns={config.replay.replay_table_name: None},
        period=config.replay.sequence_period or (config.replay.sequence_length - 1),
        sequence_length=config.replay.sequence_length,
    )
    # Variable client is responsible for syncing with the Updating node, but does not tell
    # that node when it should update.
    variable_source = services.VariableClient(source=learner)
    policy = services.LearnerPolicy(
        policy_fn=policy_graph,
        initial_state_fn=initial_state_graph,
        reverb_adder=reverb_adder,
        variable_source=variable_source,
        random_key=key0,
        backend="cpu",
    )
    players[0] = policy

    # Opponent.
    # TODO(maxsmith): This assumes that the opponent has the same graph as the player.
    opponent_variable_source = services.VariableClient(source=opponent_variable_source)
    opponent = services.EvaluationPolicy(
        policy_fn=policy_graph,
        initial_state_fn=initial_state_graph,
        variable_source=opponent_variable_source,
        random_key=key1,
        backend="cpu",
    )
    players[1] = opponent

    logger = loggers.TensorboardLogger(result_dir.dir)
    local_counter = services.Counter(parent=counter)
    train_arena = arenas.TrainingArena(
        game=game,
        players=players,
        logger=logger,
        counter=local_counter,
        step_key=config.step_key,
    )
    return train_arena


@node_utils.build_courier_node(disable_run=True)
def build_world_train_arena(config: config_dict.ConfigDict, game, players, reverb_handle):
    adder = reverb_adders.SequenceAdder(
        client=reverb_handle,
        priority_fns={config.replay_table_name: None},
        period=config.sequence_period or (config.sequence_length - 1),
        sequence_length=config.sequence_length,
    )
    return gathering_services.ModelDatasetArena(game=game, players=players, reverb_adder=adder)


@node_utils.build_courier_node(disable_run=True)
def build_agent_learner(
    step_key: str,
    frame_key: str,
    optimizer_config: config_dict.ConfigDict,
    replay_config: config_dict.ConfigDict,
    random_key: jax.random.PRNGKey,
    loss_graph: hk.Transformed,
    replay: lp.CourierHandle,
    counter: lp.CourierHandle,
    result_dir: utils.ResultDirectory,
) -> services.LearnerUpdate:
    logger = loggers.LoggerManager(
        loggers=[
            loggers.TerminalLogger(time_frequency=5, stringify_fn=_data_to_string),
            loggers.TensorboardLogger(result_dir.dir, step_key=step_key),
        ],
    )
    local_counter = services.Counter(parent=counter, time_delta=0)

    optimizer = None
    if optimizer_config.optimizer_name == "adam":
        optimizer = optax.adam
    elif optimizer_config.optimizer_name == "rmsprop":
        optimizer = optax.rmsprop
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_config.optimizer_name}.")

    grad_trans = []
    # Clip by global gradient norm.
    if optimizer_config.max_norm_steps:
        grad_trans.append(
            optax.inject_hyperparams(optax.clip_by_global_norm)(
                max_norm=optax.linear_schedule(
                    init_value=optimizer_config.max_norm_init,
                    end_value=optimizer_config.max_norm_end,
                    transition_steps=optimizer_config.max_norm_steps,
                )
            )
        )
    else:
        grad_trans.append(optax.clip_by_global_norm(max_norm=optimizer_config.max_norm_init))
    # Learning rate schedule for optimizer.
    if optimizer_config.learning_rate_steps:
        grad_trans.append(
            optax.inject_hyperparams(optimizer)(
                learning_rate=optax.linear_schedule(
                    init_value=optimizer_config.learning_rate_init,
                    end_value=optimizer_config.learning_rate_end,
                    transition_steps=optimizer_config.learning_rate_steps,
                )
            )
        )
    else:
        grad_trans.append(optimizer(learning_rate=optimizer_config.learning_rate_init))

    optimizer = optax.chain(*grad_trans)

    data_iterator = services.ReverbPrefetchClient(
        reverb_client=replay,
        table_name=replay_config.replay_table_name,
        batch_size=optimizer_config.batch_size,
    )
    return services.LearnerUpdate(
        loss_fn=loss_graph,
        optimizer=optimizer,
        data_iterator=data_iterator,
        logger=logger,
        counter=local_counter,
        step_key=step_key,
        frame_key=frame_key,
        random_key=random_key,
    )


@node_utils.build_courier_node(disable_run=True)
def build_agent_learner_with_warm_start(
    step_key: str,
    frame_key: str,
    optimizer_config: config_dict.ConfigDict,
    replay_config: config_dict.ConfigDict,
    random_key: jax.random.PRNGKey,
    loss_graph: hk.Transformed,
    replay: lp.CourierHandle,
    counter: lp.CourierHandle,
    warm_start_source: services.VariableSourceInterface,
    result_dir: utils.ResultDirectory,
) -> services.LearnerUpdate:

    logger = loggers.LoggerManager(
        loggers=[
            loggers.TerminalLogger(time_frequency=5, stringify_fn=_data_to_string),
            loggers.TensorboardLogger(result_dir.dir, step_key=step_key),
        ],
    )
    local_counter = services.Counter(parent=counter, time_delta=0)

    optimizer = None
    if optimizer_config.optimizer_name == "adam":
        optimizer = optax.adam
    elif optimizer_config.optimizer_name == "rmsprop":
        optimizer = optax.rmsprop
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_config.optimizer_name}.")

    grad_trans = []
    # Clip by global gradient norm.
    if optimizer_config.max_norm_steps:
        grad_trans.append(
            optax.inject_hyperparams(optax.clip_by_global_norm)(
                max_norm=optax.linear_schedule(
                    init_value=optimizer_config.max_norm_init,
                    end_value=optimizer_config.max_norm_end,
                    transition_steps=optimizer_config.max_norm_steps,
                )
            )
        )
    else:
        grad_trans.append(optax.clip_by_global_norm(max_norm=optimizer_config.max_norm_init))
    # Learning rate schedule for optimizer.
    if optimizer_config.learning_rate_steps:
        grad_trans.append(
            optax.inject_hyperparams(optimizer)(
                learning_rate=optax.linear_schedule(
                    init_value=optimizer_config.learning_rate_init,
                    end_value=optimizer_config.learning_rate_end,
                    transition_steps=optimizer_config.learning_rate_steps,
                )
            )
        )
    else:
        grad_trans.append(optimizer(learning_rate=optimizer_config.learning_rate_init))

    optimizer = optax.chain(*grad_trans)

    variable_source = services.VariableClient(source=warm_start_source)
    data_iterator = services.ReverbPrefetchClient(
        reverb_client=replay,
        table_name=replay_config.replay_table_name,
        batch_size=optimizer_config.batch_size,
    )
    return learner_update_with_warm_start.LearnerUpdate(
        loss_fn=loss_graph,
        optimizer=optimizer,
        data_iterator=data_iterator,
        variable_source=variable_source,
        logger=logger,
        counter=local_counter,
        step_key=step_key,
        frame_key=frame_key,
        random_key=random_key,
    )


@node_utils.build_courier_node(disable_run=True)
def build_world_model_learner(
    step_key: str,
    frame_key: str,
    random_key: jax.random.PRNGKey,
    loss_graph: hk.Transformed,
    counter_handle: lp.CourierHandle,
    reverb_handle: lp.CourierHandle,
    result_dir: utils.ResultDirectory,
    replay_config: config_dict.ConfigDict,
    optimizer_config: config_dict.ConfigDict,
):
    logger = loggers.LoggerManager(
        loggers=[
            loggers.TerminalLogger(time_frequency=5, stringify_fn=_data_to_string),
            loggers.TensorboardLogger(result_dir.dir),
        ],
    )
    local_counter = services.Counter(parent=counter_handle)

    optimizer = optax.chain(
        optax.clip_by_global_norm(optimizer_config.max_gradient_norm),
        optax.adam(optimizer_config.learning_rate),
    )

    data_iterator = services.ReverbPrefetchClient(
        reverb_client=reverb_handle,
        table_name=replay_config.replay_table_name,
        batch_size=optimizer_config.batch_size,
        buffer_size=optimizer_config.prefetch_buffer_size,
        num_threads=optimizer_config.prefetch_num_threads,
    )
    return world_model_learner.WorldModelLearner(
        loss_fn=loss_graph,
        optimizer=optimizer,
        data_iterator=data_iterator,
        logger=logger,
        counter=local_counter,
        step_key=step_key,
        frame_key=frame_key,
        random_key=random_key,
    )


@node_utils.build_courier_node(disable_run=True)
def build_render_arena_node(
    step_key: str,
    bots,
    result_dir: utils.ResultDirectory,
):
    """Builds a node that periodically renders trajectories to disk."""
    return render_arena.RenderArena(
        agents={},
        bots=bots,
        scenarios=render_arena.RenderScenario(game_ctor=build_game, num_episodes=5),
        evaluation_frequency=None,
        counter=None,  # Not running, so unneeded.
        step_key=step_key,
        result_dir=result_dir,
    )
